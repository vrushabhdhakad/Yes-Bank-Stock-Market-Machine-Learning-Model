{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "fge-S5ZAYoAp",
        "RoGjAbkUYoAp",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "578E2V7j08f6",
        "C74aWNz2AliB",
        "yiiVWRdJDDil",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**  -  Stock-Market-ML-Model\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member  ** - Vrushabh Dhakad\n",
        "\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project focuses on building a machine learning model to predict stock prices of Yes Bank using historical data. The goal was to develop a reliable, accurate, and explainable model that could assist investors and analysts in making informed financial decisions.\n",
        "\n",
        "The dataset underwent several preprocessing steps, including handling missing values, date formatting, feature extraction (like Month and Year), and feature scaling using StandardScaler. These transformations ensured the data was in the right shape and scale for effective model training.\n",
        "\n",
        "We began by exploring multiple regression models: Linear Regression, Random Forest Regressor, and AdaBoost Regressor. Each model was trained on 80% of the scaled dataset while 20% was reserved for testing. Their performances were measured using three key evaluation metrics: Mean Absolute Error (MAE), Mean Squared Error (MSE), and R² Score. These metrics were chosen for their relevance in financial forecasting—MAE gives a direct interpretation in rupees, MSE penalizes larger errors more, and R² indicates how well the model explains the variance in the stock price.\n",
        "\n",
        "Initially, Random Forest and AdaBoost outperformed Linear Regression. To further boost accuracy, we applied hyperparameter tuning using GridSearchCV for both Random Forest and AdaBoost. This process helped us identify the best combination of parameters (e.g., number of estimators, learning rate) by performing cross-validation. The tuned AdaBoost model showed the best results, achieving the lowest MAE and MSE and the highest R² Score on the test data.\n",
        "\n",
        "We also used model explainability techniques by accessing the AdaBoost Regressor’s feature importance scores. This revealed which input features—such as Open, High, Low, and Previous Close prices—had the most influence on predictions. This insight adds transparency to the model and allows stakeholders to better understand what drives stock movements.\n",
        "\n",
        "The best model (Tuned AdaBoost Regressor) was saved using joblib into a pickle file format, making it ready for deployment. To validate the save/load process, we reloaded the model and performed a sanity check by predicting on the test data again, confirming identical performance metrics.\n",
        "\n",
        "In conclusion, this end-to-end machine learning pipeline successfully demonstrated a data-driven approach to predicting Yes Bank’s stock price. From preprocessing to model evaluation and deployment, every step followed industry-standard practices. The tuned AdaBoost Regressor provides a reliable forecasting tool with strong business implications—helping users make more confident and accurate investment decisions.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem is to predict the closing price of Yes Bank stocks using historical data. Accurate predictions can help investors make informed decisions, optimize portfolios, and mitigate risks. The challenge lies in handling the volatility and non-linearity of stock prices, which are influenced by various external factors."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor , AdaBoostRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv('data_YesBank_StockPrices.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "# Check for duplicate rows\n",
        "print(\"Total duplicates:\", df.duplicated().sum())\n",
        "\n",
        "# Check for duplicate dates\n",
        "print(\"Duplicate dates:\", df['Date'].duplicated().sum())"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()  # No missing values found"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df.isnull(), cbar=False,cmap='viridis', yticklabels=False)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe() # Statistical summary"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Date        : The trading date of the stock in YYYY-MM-DD format.\n",
        "2. Open        : The price at which the stock opened on the given trading day.\n",
        "3. High        : The highest price of the stock during the trading day.\n",
        "4. Low         : The lowest price of the stock during the trading day.\n",
        "5. Close       : The actual closing price of the stock on the current trading day"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Date' to datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'],format = '%b-%y')"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract year and month for additional features\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month"
      ],
      "metadata": {
        "id": "FI8MWPz5gf8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate monthly price change\n",
        "df['Price_Change'] = df['Close'] - df['Open']"
      ],
      "metadata": {
        "id": "7OHPIKgCgyga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create lag feature: previous month's close\n",
        "df['Prev_Close'] = df['Close'].shift(1)"
      ],
      "metadata": {
        "id": "1V2-IQgfpUhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing values (due to shift)\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "svoUpS3mpaoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "CiiOD6Org0Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Histogram of Closing Price"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Close'].plot(kind='hist', bins=20, color='teal', edgecolor='black', title='Distribution of Closing Prices', figsize=(8, 4))\n",
        "plt.xlabel('Close Price')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This histogram shows how frequently different closing price ranges occurred.\n",
        "It helps in understanding the distribution and skewness of the closing price data."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the closing prices fall within a specific lower range, showing that the stock often trades at a relatively low value. This indicates a skewed distribution and suggests that high closing prices were rare events."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights from the histogram chart help create a positive business impact by showing that the stock mostly trades at a lower price range, which indicates affordability for retail investors.\n",
        "However, this also reflects that the stock rarely reaches high price points, whch might signal limited growth potential.\n",
        "This insight can help investors assess both opportunity and risk before making financial decisions.\n"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Line plot of Price Change over Time"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.set_index('Date')['Price_Change'].plot(figsize=(10, 4), title='Monthly Price Change Over Time', color='purple')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price Change')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line plot shows the monthly price change trend.\n",
        "It helps to visualize how volatile the price movement is month-to-month."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This insight suggests the stock lacks long-term growth consistency, making it risky for long-term investors but possibly useful for short-term trading strategies"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line plot of price change over time reveals frequent ups and downs in monthly price movements, indicating high volatility. This insight helps create a positive business impact by allowing traders and analysts to identify periods of increased activity, which may offer short-term profit opportunities. However, the irregular and unpredictable swings in price also reflect instability, which can lead to negative growth if investors enter during a downturn. Understanding this pattern is essential for managing risk and developing timing strategies in volatile market conditions."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Pairplot of numerical features"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df[['Open', 'High', 'Low', 'Close']])\n",
        "plt.suptitle('Pairwise Feature Relationships', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This pairplot helps to understand the relationships and scatter distributions among numerical features. It can help in spotting linear or non-linear relationships."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pairplot shows that features like Open, High, Low, and Close have strong positive relationships. The scatter plots form diagonal patterns, suggesting that these variables move together consistently. This confirms they are closely related and can be useful predictors in the model."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pairplot shows strong relationships between features like Open, High, Low, and Close. This helps create a positive business impact by confirming these variables can be used together to build accurate prediction models. However, since these features move closely together, a drop in one often affects the others too. This could lead to negative growth if early signals aren't caught, highlighting the need for timely decision-making."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Line plot"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(df['Date'], df['Close'], label='Current Close')\n",
        "plt.plot(df['Date'], df['Prev_Close'], label='Previous Close', linestyle='--')\n",
        "plt.title('Close vs Previous Close Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot compares the current closing price to the previous month's closing price.\n",
        "It helps to see how consistent or deviating the price was from the last month."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line plot shows that the closing price of Yes Bank is highly volatile with no clear upward trend. This suggests that the stock lacks long-term growth stability and may be more suited for short-term trading rather than long-term investment.\n",
        "\n"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line plot reveals that Yes Bank's closing prices are highly volatile with no clear upward trend. This can help create a positive business impact by allowing traders to spot short-term opportunities and manage risk better. However, the frequent fluctuations and absence of long-term growth also raise concerns, as they indicate potential instability and negative growth for long-term investors."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bar chart"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('Year')['Close'].mean().plot(kind='bar', figsize=(8, 4), title='Average Close Price Per Year', color='skyblue')\n",
        "plt.ylabel('Average Close Price')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart shows how the average closing price changes each year. it helps detect any year-over-year trends in stock performance."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bar chart shows fluctuations in the average closing price of Yes Bank across different years. It highlights that the stock did not show consistent growth year over year, with some years experiencing a decline. This suggests unstable long-term performance and helps identify which years had relatively better or worse stock value."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bar chart helps identify how the average closing price of Yes Bank changed each year. This insight creates a positive business impact by highlighting which years performed better, guiding future investment timing. However, the inconsistency and decline in some years indicate potential negative growth and raise concerns about the stock's long-term reliability.\n",
        "\n"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Box plot"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x='Month', y='Close', data=df, palette='Set2')\n",
        "plt.title('Monthly Close Price Distribution')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A box plot reveals the distribution and outliers of close prices for each month.\n",
        "It helps identify seasonal effects or consistency in monthly prices."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The box plot shows that the distribution of closing prices varies significantly across different months. Some months have wider price ranges and outliers, indicating high volatility, while others are more stable. This suggests that certain months may be riskier or more active for trading than others.\n",
        "\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The box plot highlights monthly variations in closing prices, revealing which months are more volatile and which are stable. This helps create a positive business impact by guiding investors to choose more predictable months for trading. However, the presence of outliers and wide ranges in some months also points to unpredictability and risk, which can lead to negative outcomes if not carefully monitored."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scatter plot"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x='Open', y='Close', data=df)\n",
        "plt.title('Open vs Close Price')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot shows the relationship between opening and closing prices.\n",
        "A strong upward trend here supports high correlation between these features."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatter plot shows a strong positive relationship between the opening and closing prices. This means that when the stock opens high, it generally closes high, indicating consistent intraday movement."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatter plot shows a strong positive relationship between opening and closing prices, which supports more accurate model predictions and helps traders anticipate daily stock behavior, creating a positive business impact. However, if the stock opens low, it’s likely to close low as well, which may signal potential negative growth, especially during bearish market conditions."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.set_index('Date')['Close'].plot(figsize=(10, 4), title='Yes Bank Closing Price Over Time', grid=True)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line plot was chosen because it clearly shows the trend and volatility in Yes Bank’s closing price over time. It helps in understanding whether the stock follows a stable growth pattern or experiences frequent fluctuations, which is crucial for investment decision-making.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line plot shows that Yes Bank’s closing price is highly volatile over time without a consistent upward trend. This suggests the stock lacks long-term stability but may offer opportunities for short-term trading during price swings.\n",
        "\n"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line plot shows that Yes Bank’s closing prices are highly volatile with no clear long-term growth. This helps create a positive business impact by informing investors about the risky nature of the stock, encouraging more careful short-term strategies. However, the lack of consistent upward movement and frequent dips also signal instability, which could lead to negative growth if not managed properly, especially for long-term investments.\n",
        "\n"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Countplot"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='Month', data=df, palette='Set3')\n",
        "plt.title('Monthly Count of Closing Records')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The countplot was chosen because it helps visualize how often the stock closed in each month. It’s useful for identifying seasonal patterns and understanding if certain months are more active or consistent in trading volume."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The countplot shows that certain months have more frequent closing records than others, suggesting possible seasonal activity or more consistent trading behavior in those months. This can help detect if some months are more stable or active than the rest."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the countplot helps identify which months have more frequent trading activity, which can guide investors to focus on more stable periods. This creates a positive business impact by revealing seasonal trends that may align with market performance. However, if certain months show consistently low activity, it may indicate investor hesitation or negative sentiment, which could signal potential risk or negative growth during those times.\n",
        "\n"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Line Plot"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_monthly_close = df.groupby('Month')['Close'].mean()\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(avg_monthly_close.index, avg_monthly_close.values, marker='o', color='green')\n",
        "plt.title('Average Close Price by Month')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Average Close Price')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart helps to see if there is any trend in average closing price across months."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line plot shows that the average closing price fluctuates across different months. Some months have slightly higher averages, while others dip, indicating that the stock doesn't follow a consistent seasonal trend. This helps understand monthly behavior for better timing of investments."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart helps create a positive business impact by revealing how the average closing price changes from month to month, which supports better timing for investment decisions. However, the fluctuations across months show a lack of seasonal stability, which may lead to negative growth if investors rely on monthly patterns that are inconsistent."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bar Plot"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Direction'] = df['Price_Change'].apply(lambda x: 'Positive' if x > 0 else 'Negative')\n",
        "sns.countplot(x='Direction', data=df, palette='coolwarm')\n",
        "plt.title('Frequency of Monthly Price Change Direction')\n",
        "plt.xlabel('Direction')\n",
        "plt.ylabel('Count')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart shows how many times the stock had positive vs negative price change in a month."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows that there are more months with negative price changes than positive ones. This indicates that the stock experienced downward movement more frequently, which reflects a bearish or unstable trend over time.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart helps create a positive business impact by showing how often the stock had positive or negative monthly returns, helping investors assess risk. However, since negative months are more frequent, it signals instability and potential negative growth, especially for long-term investments.\n",
        "\n"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Correlation heatmap"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(df[['Open', 'High', 'Low', 'Close', 'Prev_Close']].corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Feature Correlation')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation heatmap was chosen because it visually shows how strongly different features are related to each other. It helps quickly identify which variables are useful predictors for the target feature, especially the closing price."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmap shows that features like Open, High, Low, and Prev_Close have a strong positive correlation with Close. This confirms they are important predictors and can be used effectively in the model to improve closing price prediction."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation heatmap helps create a positive business impact by highlighting that features like Open, High, Low, and Prev_Close are strongly correlated with the Close price, making them reliable for building predictive models. However, since these features move together, any sudden drop in one may lead to a chain reaction, increasing the risk of negative growth during market downturns."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Is there a significant difference between monthly average Open and Close prices?"
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Null Hypothesis (H₀): There is no significant difference between the average Open and Close prices.\n",
        "\n",
        "*   Open and Close prices.\n",
        "Alternate Hypothesis (H₁): There is a significant difference between the average Open and Close prices.\n",
        "\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_rel\n",
        "\n",
        "# Grouping by Month to compute monthly averages\n",
        "monthly_avg = df.groupby('Month')[['Open', 'Close']].mean()\n",
        "\n",
        "# Perform paired t-test\n",
        "stat, p_value = ttest_rel(monthly_avg['Open'], monthly_avg['Close'])\n",
        "print(\"Hypothesis Test - Monthly Average Open vs Close\")\n",
        "print(\"T-statistic:\", stat)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Conclusion: Reject the null hypothesis. There is a significant difference between monthly average Open and Close prices.\")\n",
        "else:\n",
        "    print(\"Conclusion: Fail to reject the null hypothesis. There is no significant difference between monthly average Open and Close prices.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To obtain the p-value for Hypothetical Statement 1, we used a Paired Sample t-test. This statistical test is suitable because we are comparing two related variables — the monthly average Open and Close prices. The test checks if there is a significant difference between their means. The resulting p-value helps us decide whether to reject the null hypothesis."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "because we are comparing two related values the monthly average Open and Close prices for the same periods. This test is appropriate when we want to check if there is a significant difference between two dependent groups, and it helps determine if the average price changed meaningfully over time."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Do years with higher average trading volume also have higher average closing prices?"
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* Null Hypothesis (H₀): There is no correlation between yearly average Volume and average Close price.\n",
        "* Alternate Hypothesis (H₁): There is a significant correlation between yearly average Volume and average Close price.\n",
        "\n"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Grouping by year to compute average Close and High prices\n",
        "yearly_avg = df.groupby('Year')[['High', 'Close']].mean()\n",
        "\n",
        "# Perform Statistical Test to obtain P-Value\n",
        "corr_stat, p_val = pearsonr(yearly_avg['High'], yearly_avg['Close'])\n",
        "print(\"\\nHypothesis Test - Correlation between High and Close\")\n",
        "print(\"Correlation Coefficient:\", corr_stat)\n",
        "print(\"P-value:\", p_val)\n",
        "\n",
        "# Interpret results\n",
        "if p_val < 0.05:\n",
        "    print(\"Conclusion: Reject the null hypothesis. There is a significant correlation between average yearly high and closing prices.\")\n",
        "else:\n",
        "    print(\"Conclusion: Fail to reject the null hypothesis. There is no significant correlation between average yearly high and closing prices.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Pearson Correlation Test was used to check whether there is a significant linear relationship between average yearly trading volume and average yearly closing price.This test is suitable for evaluating the strength and direction of the linear relationship between two continuous numerical variables.\n"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose the Pearson Correlation Test because it is ideal for measuring the strength and direction of the linear relationship between two continuous variables  in this case, the average yearly trading volume and average yearly closing price. This test helps us understand whether higher trading activity is associated with changes in closing price, which is useful for identifying influential predictors in stock behavior."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Has the average monthly price change significantly increased in recent years compared to earlier years?"
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "*  Null Hypothesis (H₀): There is no significant difference in average monthly price change between earlier years and recent years.\n",
        "\n",
        "*   Alternate Hypothesis (H₁): There is a significant difference in average monthly price change between earlier years and recent years.\n",
        "\n"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Categorize data into earlier years and recent years\n",
        "df['Period'] = df['Year'].apply(lambda x: 'Earlier' if x < 2020 else 'Recent')\n",
        "\n",
        "# Calculate average price change per month for both periods\n",
        "earlier = df[df['Period'] == 'Earlier']['Price_Change']\n",
        "recent = df[df['Period'] == 'Recent']['Price_Change']\n",
        "\n",
        "# Perform independent t-test\n",
        "stat, p_val = ttest_ind(earlier, recent)\n",
        "print(\"Hypothesis Test - Average Monthly Price Change: Earlier vs Recent\")\n",
        "print(\"T-statistic:\", stat)\n",
        "print(\"P-value:\", p_val)\n",
        "\n",
        "if p_val < 0.05:\n",
        "    print(\"Conclusion: Reject the null hypothesis. There is a significant difference in average monthly price change between earlier and recent years.\")\n",
        "else:\n",
        "    print(\"Conclusion: Fail to reject the null hypothesis. No significant difference in average monthly price change between earlier and recent years.\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the Independent t-test to compare the average monthly price change between earlier years and recent years. This test is suitable for checking if there is a significant difference in means between two independent groups.\n",
        "\n"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent t-test because it helps compare the means of two separate time periods — earlier and recent years — to check if the average monthly price change has significantly shifted over time. It’s suitable when analyzing differences between two independent groups."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values before dropping\n",
        "total_missing_before = df.isnull().sum()\n",
        "print(\"Missing values before dropping:\\n\", total_missing_before)\n",
        "\n",
        "# Drop missing values caused by lag feature or others\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Check again after dropping missing values\n",
        "total_missing_after = df.isnull().sum()\n",
        "print(\"Missing values after dropping:\\n\", total_missing_after)\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "handled missing values using row removal with df.dropna() because the only missing values were introduced by the creation of the lag feature Prev_Close, which shifted data by one row. Since this caused only the first row to have NaN, removing it ensured a clean dataset without significantly affecting data volume. No further imputation (mean/median filling) was necessary as there were no other missing values in the dataset."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll detect outliers in 'Close' prices using the IQR method.\n",
        "Q1 = df['Close'].quantile(0.25)\n",
        "Q3 = df['Close'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define lower and upper bounds\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Filter data within bounds\n",
        "df = df[(df['Close'] >= lower_bound) & (df['Close'] <= upper_bound)]\n",
        "\n",
        "print(\"Outliers removed based on IQR in 'Close' prices.\")\n",
        "\n",
        "df.head(10)\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IQR (Interquartile Range) method to detect and remove outliers from the Close price column. This technique is effective in identifying data points that fall significantly outside the typical range. It was chosen because it is simple, robust to skewed data, and does not assume any distribution shape, making it suitable for financial datasets like stock prices that often have extreme values.\n",
        "\n"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode 'Direction' (Positive/Negative) and 'Period' (Earlier/Recent)\n",
        "le = LabelEncoder()\n",
        "df['Direction_Encoded'] = le.fit_transform(df['Direction'])\n",
        "df['Period_Encoded'] = le.fit_transform(df['Period'])\n",
        "\n",
        "print(\"Categorical features 'Direction' and 'Period' encoded using Label Encoding.\")\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Encoding on the Direction and Period columns because they contain binary categories (e.g., Positive/Negative, Earlier/Recent). Label Encoding is efficient for such cases and helps convert these text labels into numerical values so they can be used in machine learning models without increasing dimensionality, unlike One-Hot Encoding."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We selected the features `['Open', 'High', 'Low', 'Prev_Close', 'Month', 'Year']`\n",
        "# based on their strong correlation with the target variable `Close`."
      ],
      "metadata": {
        "id": "XdFNAv8kaC3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Created a new feature Price_Change to capture the monthly price movement by calculating the difference between Close and Open.\n",
        "\n",
        "#Generated Prev_Close using .shift(1) to include the previous month's closing price, which adds historical context to the current data.\n",
        "\n",
        "#Dropped the first row using dropna() to remove the missing value caused by the lag feature.\n",
        "\n",
        "#These manipulated features help the model learn from stock movement patterns and past trends for better prediction."
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['Open', 'High', 'Low', 'Prev_Close', 'Month', 'Year']\n",
        "target = 'Close'\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used a combination of domain knowledge and correlation analysis to select relevant features. Features like Open, High, Low, and Prev_Close were selected due to their strong correlation with the target variable Close, as observed in the heatmap. Additionally, Month and Year were included to capture seasonal and time-based trends. This approach helps avoid overfitting by excluding irrelevant or redundant features."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The important features for predicting Yes Bank's stock price are Open, High, Low, Prev_Close, Month, and Year. These were chosen because they reflect market behavior, recent trends, and seasonal patterns—all of which significantly influence the stock’s closing price."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We used StandardScaler from sklearn.preprocessing to scale our features.\n",
        "#StandardScaler standardizes the data by removing the mean and scaling to unit variance,\n",
        "#resulting in a distribution with a mean of 0 and a standard deviation of 1.\n",
        "#This is especially helpful for models like Linear Regression which are sensitive to the scale of input features.\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction is not required in this project because the number of features used (6 in total) is already quite small and manageable.\n",
        "All features included are relevant and meaningful for predicting the stock price.\n",
        "Using dimensionality reduction techniques like PCA would not provide significant benefit here and might result in loss of important information."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, shuffle=False, test_size=0.2)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used an 80:20 split ratio for training and testing the dataset.\n",
        "This ratio ensures that the model has sufficient data (80%) to learn patterns effectively,\n",
        "while reserving a fair portion (20%) for evaluating its performance on unseen data.\n",
        "Also, we set shuffle=False to preserve the chronological order of stock prices,\n",
        "as it is a time series problem."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, the dataset is not imbalanced in this case.\n",
        "\n",
        "Imbalance is typically a concern in classification problems where one class significantly outnumbers the other, leading to biased model predictions. However, this project is a regression task where the target variable is a continuous numeric value , not categorical.\n",
        "\n",
        "Here, we are predicting stock prices, not classifying them. Therefore, the concept of imbalance doesn’t apply in the same way, and there is no indication that certain price ranges are overly dominant or underrepresented in a way that would bias the model."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "lr_preds = lr_model.predict(X_test)\n",
        "rf_preds = rf_model.predict(X_test)\n",
        "\n",
        "print(\"\\nLinear Regression Performance:\")\n",
        "print(\"MAE:\", mean_absolute_error(y_test, lr_preds))\n",
        "print(\"MSE:\", mean_squared_error(y_test, lr_preds))\n",
        "print(\"R2 Score:\", r2_score(y_test, lr_preds))\n",
        "\n",
        "print(\"\\nRandom Forest Regressor Performance:\")\n",
        "print(\"MAE:\", mean_absolute_error(y_test, rf_preds))\n",
        "print(\"MSE:\", mean_squared_error(y_test, rf_preds))\n",
        "print(\"R2 Score:\", r2_score(y_test, rf_preds))\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df = pd.DataFrame({\n",
        "    'Model': ['Linear Regression', 'Random Forest'],\n",
        "    'MAE': [mean_absolute_error(y_test, lr_preds), mean_absolute_error(y_test, rf_preds)],\n",
        "    'MSE': [mean_squared_error(y_test, lr_preds), mean_squared_error(y_test, rf_preds)],\n",
        "    'R2 Score': [r2_score(y_test, lr_preds), r2_score(y_test, rf_preds)]\n",
        "})\n",
        "\n",
        "metrics_df.set_index('Model', inplace=True)\n",
        "metrics_df.plot(kind='bar', figsize=(10, 6))\n",
        "plt.title('Evaluation Metric Score Comparison')\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AgH86KNBmLYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5, scoring='r2')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nBest Hyperparameters:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Fit the best estimator\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Predict on the test set\n",
        "best_rf_preds = best_rf.predict(X_test)\n",
        "\n",
        "print(\"\\nOptimized Random Forest Performance:\")\n",
        "print(\"MAE:\", mean_absolute_error(y_test, best_rf_preds))\n",
        "print(\"MSE:\", mean_squared_error(y_test, best_rf_preds))\n",
        "print(\"R2 Score:\", r2_score(y_test, best_rf_preds))\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used GridSearchCV for hyperparameter tuning because it performs an exhaustive search over the specified parameter grid and selects the best combination based on cross-validation, which works well given the small dataset size and ensures optimal performance."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, we observed improvement after hyperparameter tuning using GridSearchCV.\n",
        "\n",
        "The optimized Random Forest model achieved better performance metrics compared to the default model. Specifically, the MAE and MSE decreased, and the R² Score increased, indicating that the model is now making more accurate predictions."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Evaluation Metric Score Chart\n",
        "metrics_df_updated = pd.DataFrame({\n",
        "    'Model': ['Linear Regression', 'Random Forest (Default)', 'Random Forest (Tuned)'],\n",
        "    'MAE': [\n",
        "        mean_absolute_error(y_test, lr_preds),\n",
        "        mean_absolute_error(y_test, rf_preds),\n",
        "        mean_absolute_error(y_test, best_rf_preds)\n",
        "    ],\n",
        "    'MSE': [\n",
        "        mean_squared_error(y_test, lr_preds),\n",
        "        mean_squared_error(y_test, rf_preds),\n",
        "        mean_squared_error(y_test, best_rf_preds)\n",
        "    ],\n",
        "    'R2 Score': [\n",
        "        r2_score(y_test, lr_preds),\n",
        "        r2_score(y_test, rf_preds),\n",
        "        r2_score(y_test, best_rf_preds)\n",
        "    ]\n",
        "})\n",
        "\n",
        "metrics_df_updated.set_index('Model', inplace=True)\n",
        "metrics_df_updated.plot(kind='bar', figsize=(12, 6))\n",
        "plt.title('Updated Evaluation Metric Score Comparison')\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LmBZHIh1rQUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gbr_model.fit(X_train, y_train)\n",
        "gbr_preds = gbr_model.predict(X_test)\n",
        "\n",
        "print(\"\\nGradient Boosting Regressor Performance:\")\n",
        "print(\"MAE:\", mean_absolute_error(y_test, gbr_preds))\n",
        "print(\"MSE:\", mean_squared_error(y_test, gbr_preds))\n",
        "print(\"R2 Score:\", r2_score(y_test, gbr_preds))\n"
      ],
      "metadata": {
        "id": "68FeVKjFr7ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Model': ['Linear Regression', 'Random Forest', 'Optimized RF', 'Gradient Boosting'],\n",
        "    'MAE': [\n",
        "        mean_absolute_error(y_test, lr_preds),\n",
        "        mean_absolute_error(y_test, rf_preds),\n",
        "        mean_absolute_error(y_test, best_rf_preds),\n",
        "        mean_absolute_error(y_test, gbr_preds)\n",
        "    ],\n",
        "    'MSE': [\n",
        "        mean_squared_error(y_test, lr_preds),\n",
        "        mean_squared_error(y_test, rf_preds),\n",
        "        mean_squared_error(y_test, best_rf_preds),\n",
        "        mean_squared_error(y_test, gbr_preds)\n",
        "    ],\n",
        "    'R2 Score': [\n",
        "        r2_score(y_test, lr_preds),\n",
        "        r2_score(y_test, rf_preds),\n",
        "        r2_score(y_test, best_rf_preds),\n",
        "        r2_score(y_test, gbr_preds)\n",
        "    ]\n",
        "})\n",
        "\n",
        "metrics_df.set_index('Model', inplace=True)\n",
        "metrics_df.plot(kind='bar', figsize=(12, 6))\n",
        "plt.title('Evaluation Metric Score Comparison Across All Models')\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5, scoring='r2')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nBest Hyperparameters:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "best_rf = grid_search.best_estimator_\n",
        "best_rf_preds = best_rf.predict(X_test)\n",
        "\n",
        "print(\"\\nOptimized Random Forest Performance:\")\n",
        "print(\"MAE:\", mean_absolute_error(y_test, best_rf_preds))\n",
        "print(\"MSE:\", mean_squared_error(y_test, best_rf_preds))\n",
        "print(\"R2 Score:\", r2_score(y_test, best_rf_preds))\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used GridSearchCV for hyperparameter optimization because it systematically searches across a predefined grid of parameters and uses cross-validation to find the best combination. It ensures optimal tuning for the model’s performance, especially effective here due to the manageable dataset size and need for precise parameter selection."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = ['Linear Regression', 'Random Forest (Default)', 'Random Forest (Tuned)']\n",
        "mae_scores = [\n",
        "    mean_absolute_error(y_test, lr_preds),\n",
        "    mean_absolute_error(y_test, rf_preds),\n",
        "    mean_absolute_error(y_test, best_rf_preds)\n",
        "]\n",
        "mse_scores = [\n",
        "    mean_squared_error(y_test, lr_preds),\n",
        "    mean_squared_error(y_test, rf_preds),\n",
        "    mean_squared_error(y_test, best_rf_preds)\n",
        "]\n",
        "r2_scores = [\n",
        "    r2_score(y_test, lr_preds),\n",
        "    r2_score(y_test, rf_preds),\n",
        "    r2_score(y_test, best_rf_preds)\n",
        "]\n",
        "\n",
        "x = np.arange(len(models))\n",
        "width = 0.25\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "rects1 = ax.bar(x - width, mae_scores, width, label='MAE')\n",
        "rects2 = ax.bar(x, mse_scores, width, label='MSE')\n",
        "rects3 = ax.bar(x + width, r2_scores, width, label='R2 Score')\n",
        "\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Evaluation Metric Score Comparison')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models)\n",
        "ax.legend()\n",
        "\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "G0KlAjDUuE2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* MAE (Mean Absolute Error):\n",
        "Indicates the average absolute difference between predicted and actual stock prices. Lower MAE means the model makes fewer mistakes in rupee terms, helping investors make more precise decisions.\n",
        "\n",
        "* MSE (Mean Squared Error):\n",
        "Penalizes larger errors more than MAE. A lower MSE means fewer big mistakes, which reduces the risk of incorrect stock price predictions, improving trust in the model for financial planning.\n",
        "\n",
        "* R² Score (Coefficient of Determination):\n",
        "Represents how well the model explains the variability of stock prices. A higher R² means the model captures trends effectively, leading to more confident and data-driven investment strategies.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ada_model = AdaBoostRegressor(n_estimators=100, random_state=42)\n",
        "ada_model.fit(X_train, y_train)\n",
        "ada_preds = ada_model.predict(X_test)\n",
        "\n",
        "print(\"\\nAdaBoost Regressor Performance:\")\n",
        "print(\"MAE:\", mean_absolute_error(y_test, ada_preds))\n",
        "print(\"MSE:\", mean_squared_error(y_test, ada_preds))\n",
        "print(\"R2 Score:\", r2_score(y_test, ada_preds))"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "models = ['Linear Regression', 'Random Forest (Default)', 'Random Forest (Tuned)', 'AdaBoost']\n",
        "mae_scores = [\n",
        "    mean_absolute_error(y_test, lr_preds),\n",
        "    mean_absolute_error(y_test, rf_preds),\n",
        "    mean_absolute_error(y_test, best_rf_preds),\n",
        "    mean_absolute_error(y_test, ada_preds)\n",
        "]\n",
        "mse_scores = [\n",
        "    mean_squared_error(y_test, lr_preds),\n",
        "    mean_squared_error(y_test, rf_preds),\n",
        "    mean_squared_error(y_test, best_rf_preds),\n",
        "    mean_squared_error(y_test, ada_preds)\n",
        "]\n",
        "r2_scores = [\n",
        "    r2_score(y_test, lr_preds),\n",
        "    r2_score(y_test, rf_preds),\n",
        "    r2_score(y_test, best_rf_preds),\n",
        "    r2_score(y_test, ada_preds)\n",
        "]\n",
        "\n",
        "x = np.arange(len(models))\n",
        "width = 0.25\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 7))\n",
        "rects1 = ax.bar(x - width, mae_scores, width, label='MAE')\n",
        "rects2 = ax.bar(x, mse_scores, width, label='MSE')\n",
        "rects3 = ax.bar(x + width, r2_scores, width, label='R2 Score')\n",
        "\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Evaluation Metric Score Comparison Across All Models')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models)\n",
        "ax.legend()\n",
        "\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Algorithm\n",
        "ada_param_grid = {\n",
        "    'n_estimators': [50, 100, 150, 200],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0]\n",
        "}\n",
        "\n",
        "ada_grid = GridSearchCV(AdaBoostRegressor(random_state=42), ada_param_grid, cv=5, scoring='r2')\n",
        "ada_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nBest AdaBoost Hyperparameters:\")\n",
        "print(ada_grid.best_params_)\n",
        "\n",
        "# Predict on the model\n",
        "best_ada = ada_grid.best_estimator_\n",
        "best_ada_preds = best_ada.predict(X_test)\n",
        "\n",
        "print(\"\\nOptimized AdaBoost Performance:\")\n",
        "print(\"MAE:\", mean_absolute_error(y_test, best_ada_preds))\n",
        "print(\"MSE:\", mean_squared_error(y_test, best_ada_preds))\n",
        "print(\"R2 Score:\", r2_score(y_test, best_ada_preds))"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV as the hyperparameter optimization technique because it systematically explores all combinations of specified parameters using cross-validation. This ensures we find the best set of parameters for AdaBoost Regressor, leading to improved model accuracy and generalization."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = ['Linear Regression', 'Random Forest (Default)', 'Random Forest (Tuned)', 'AdaBoost (Default)', 'AdaBoost (Tuned)']\n",
        "mae_scores = [\n",
        "    mean_absolute_error(y_test, lr_preds),\n",
        "    mean_absolute_error(y_test, rf_preds),\n",
        "    mean_absolute_error(y_test, best_rf_preds),\n",
        "    mean_absolute_error(y_test, ada_preds),\n",
        "    mean_absolute_error(y_test, best_ada_preds)\n",
        "]\n",
        "mse_scores = [\n",
        "    mean_squared_error(y_test, lr_preds),\n",
        "    mean_squared_error(y_test, rf_preds),\n",
        "    mean_squared_error(y_test, best_rf_preds),\n",
        "    mean_squared_error(y_test, ada_preds),\n",
        "    mean_squared_error(y_test, best_ada_preds)\n",
        "]\n",
        "r2_scores = [\n",
        "    r2_score(y_test, lr_preds),\n",
        "    r2_score(y_test, rf_preds),\n",
        "    r2_score(y_test, best_rf_preds),\n",
        "    r2_score(y_test, ada_preds),\n",
        "    r2_score(y_test, best_ada_preds)\n",
        "]\n",
        "\n",
        "x = np.arange(len(models))\n",
        "width = 0.25\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16, 7))\n",
        "rects1 = ax.bar(x - width, mae_scores, width, label='MAE')\n",
        "rects2 = ax.bar(x, mse_scores, width, label='MSE')\n",
        "rects3 = ax.bar(x + width, r2_scores, width, label='R2 Score')\n",
        "\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Updated Evaluation Metric Score Comparison Across All Models')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models)\n",
        "ax.legend()\n",
        "\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9HY2suhnylUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I considered three key evaluation metrics: MAE (Mean Absolute Error), MSE (Mean Squared Error), and R² Score. Among these, R² Score was the most impactful for business decisions because it explains the proportion of variance in stock prices captured by the model. A high R² indicates better reliability in predictions, which is crucial for investors. Additionally, MAE was preferred over MSE for understanding average prediction errors in real currency terms, providing more interpretable and actionable insights for financial planning."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ML Model - 3: Tuned AdaBoost Regressor as the final prediction model because it outperformed the other models in terms of evaluation metrics. After applying hyperparameter tuning using GridSearchCV, this model achieved the lowest MAE and MSE and the highest R² Score compared to ML Model - 1 (Linear Regression), ML Model - 2 (Random Forest Regressor, both default and tuned). These improvements make ML Model - 3 more reliable for forecasting stock prices, which is essential for generating accurate financial insights."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The Tuned AdaBoost Regressor model, which is an ensemble technique that combines several weak learners to form a strong predictive model. It works by focusing more on the data points that are harder to predict, improving accuracy over multiple iterations.\n",
        "\n",
        "To understand how the model makes decisions, we used its built-in feature importance method. This helps identify which features most influence the model’s predictions. In our case, features like “Open”, “High”, “Low”, and “Prev_Close” prices were among the most important. This insight is valuable for the business as it highlights which variables impact stock price movements the most, aiding better forecasting and strategic decision-making."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import joblib\n",
        "joblib.dump(best_ada, 'best_adaboost_model.pkl')"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model for sanity check\n",
        "loaded_model = joblib.load('best_adaboost_model.pkl')\n",
        "\n",
        "# Predict on unseen test data using the loaded model\n",
        "sanity_preds = loaded_model.predict(X_test)\n",
        "\n",
        "print(\"\\nSanity Check Prediction on Unseen Data:\")\n",
        "print(\"MAE:\", mean_absolute_error(y_test, sanity_preds))\n",
        "print(\"MSE:\", mean_squared_error(y_test, sanity_preds))\n",
        "print(\"R2 Score:\", r2_score(y_test, sanity_preds))"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After evaluating multiple models including Linear Regression, Random Forest, and AdaBoost, we selected the Tuned AdaBoost Regressor as the final model based on its superior performance metrics.\n",
        "With the help of GridSearchCV, we optimized its hyperparameters which significantly improved prediction accuracy.\n",
        "The model showed the lowest MAE and MSE along with the highest R² score among all.\n",
        "This model is now saved for deployment and successfully passed a sanity check on unseen test data, making it reliable for forecasting Yes Bank stock prices."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}